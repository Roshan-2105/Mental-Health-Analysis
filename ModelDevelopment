#Preprocessdata
forintentindata['intents']:
for
pattern in intent['patterns']: wrds=nltk.word_tokenize(pattern) words.extend(wrds) docs_x.append(wrds) docs_y.append(intent["tag"])
ifintent['tag']notinlabels: labels.append(intent['tag'])
words=[stemmer.stem(w.lower())forwinwordsifw!="?"] words=sorted(list(set(words)))
labels = sorted(labels)
training=[] output =[]
out_empty = [0] * len(labels)
forx,docinenumerate(docs_x): bag=[]
wrds=[stemmer.stem(w.lower())forwindoc] for w in words:
bag.append(1)ifwinwrdselsebag.append(0) output_row = out_empty[:] output_row[labels.index(docs_y[x])]=1 training.append(bag) output.append(output_row)
training=np.array(training) output=np.array(output)
